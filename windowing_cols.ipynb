{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "from category_encoders import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in ./my-python3-env/lib/python3.9/site-packages (from category_encoders) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in ./my-python3-env/lib/python3.9/site-packages (from category_encoders) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in ./my-python3-env/lib/python3.9/site-packages (from category_encoders) (1.12.0)\n",
      "Collecting statsmodels>=0.9.0 (from category_encoders)\n",
      "  Downloading statsmodels-0.14.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: pandas>=1.0.5 in ./my-python3-env/lib/python3.9/site-packages (from category_encoders) (2.1.4)\n",
      "Collecting patsy>=0.5.1 (from category_encoders)\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./my-python3-env/lib/python3.9/site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./my-python3-env/lib/python3.9/site-packages (from pandas>=1.0.5->category_encoders) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./my-python3-env/lib/python3.9/site-packages (from pandas>=1.0.5->category_encoders) (2023.3)\n",
      "Requirement already satisfied: six in ./my-python3-env/lib/python3.9/site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./my-python3-env/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./my-python3-env/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category_encoders) (3.3.0)\n",
      "Requirement already satisfied: packaging>=21.3 in ./my-python3-env/lib/python3.9/site-packages (from statsmodels>=0.9.0->category_encoders) (23.2)\n",
      "Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m862.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.9/233.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading statsmodels-0.14.2-cp39-cp39-macosx_10_9_x86_64.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: patsy, statsmodels, category_encoders\n",
      "Successfully installed category_encoders-2.6.3 patsy-0.5.6 statsmodels-0.14.2\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngine:\n",
    "    def __init__(self,):\n",
    "        self.training_set = pd.read_csv(\"cleaned_data/train_data.csv\")\n",
    "        self.validation_set = pd.read_csv(\"cleaned_data/validation_data.csv\")\n",
    "        self.test_set = pd.read_csv(\"cleaned_data/test_data.csv\")\n",
    "        self.demographic_set = pd.read_csv(\"Data Tables/HScreening.txt\", delimiter = '|')\n",
    "        #self.time_series = pd.read_csv(\"Experimental_Notebooks/resampled_day.csv\")\n",
    "        self.aggregate_window(self.tar, \"tar\")\n",
    "        #self.aggregate_window(self.tbr, \"tbr\")\n",
    "        #self.aggregate_window(self.rolling_mean, \"mean\")\n",
    "        #self.aggregate_window(self.rolling_deviation, \"std\")\n",
    "        #self.aggregate_window(self.tir, \"tir\")\n",
    "        #self.aggregate_window(self.fft, \"fft\")\n",
    "        #self.add_demographics()\n",
    "\n",
    "\n",
    "    \n",
    "    def fft(self, arr:np.ndarray) -> np.ndarray:\n",
    "        return fft(arr)\n",
    "    \n",
    "    def rolling_deviation(self, arr:np.ndarray) -> np.ndarray:\n",
    "        return np.std(arr)\n",
    "    \n",
    "    def tar(self, arr: np.ndarray) -> np.ndarray:\n",
    "        mask = arr > 180\n",
    "        return np.mean(mask)\n",
    "    \n",
    "    def tir(self, arr: np.ndarray) -> np.ndarray:\n",
    "        mask = (arr >= 70) & (arr <= 180)\n",
    "        return np.mean(mask)\n",
    "    \n",
    "    def tbr(self, arr: np.ndarray) -> np.ndarray:\n",
    "        mask = arr < 70\n",
    "        return np.mean(mask)\n",
    "    \n",
    "    def rolling_mean(self, arr: np.ndarray) -> np.ndarray:\n",
    "        return np.mean(arr)\n",
    "\n",
    "    def add_demographics(self) -> None:\n",
    "        \"\"\"\n",
    "        Merge selected demographic features into the training, validation, and test sets.\n",
    "        Encode them using one-hot encoding.\n",
    "        \"\"\"\n",
    "        # Columns to merge\n",
    "        columns_to_merge = [\n",
    "            'PtID', 'Gender', 'Ethnicity', 'Race', 'SHMostRec', 'SHNumLast12Mon', 'DKAMostRec', 'DKANumLast12Mon',\n",
    "            'OthGlucLowerMed', 'Weight', 'Height', 'PEAbnormal'\n",
    "        ]\n",
    "\n",
    "        # Filter the demographic set to include only the necessary columns\n",
    "        demographics = self.demographic_set[columns_to_merge]\n",
    "\n",
    "        # Prepare the encoder\n",
    "        encoder = OneHotEncoder(cols=['Gender', 'Ethnicity', 'Race', 'SHMostRec', 'SHNumLast12Mon', 'DKAMostRec', 'DKANumLast12Mon', 'OthGlucLowerMed', 'PEAbnormal'], use_cat_names=True)\n",
    "\n",
    "        # Fit and transform the encoder on the demographic data\n",
    "        demographics_encoded = encoder.fit_transform(demographics)\n",
    "\n",
    "        # Merge the encoded demographic data with each dataset\n",
    "        self.training_set = self.training_set.merge(demographics_encoded, how='left', left_on='id', right_on='PtID')\n",
    "        self.validation_set = self.validation_set.merge(demographics_encoded, how='left', left_on='id', right_on='PtID')\n",
    "        self.test_set = self.test_set.merge(demographics_encoded, how='left', left_on='id', right_on='PtID')\n",
    "\n",
    "    def aggregate_window(self, func: Callable, func_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Apply func over various window sizes and add columns to training and validation sets for the output of these aggregate functions\n",
    "        \n",
    "        Args:\n",
    "        func (Callable): A function to apply to each aggregate window.\n",
    "        func_name (str): The name of the function, used to create new column names.\n",
    "        \"\"\"\n",
    "        # Define the time intervals in minutes for aggregation\n",
    "        time_windows = {\n",
    "            'last_10_minutes': 10,\n",
    "            'last_30_minutes': 30,\n",
    "            'last_1_hour': 60,\n",
    "            'last_3_hours': 180,\n",
    "            'last_6_hours': 360,\n",
    "            'last_12_hours': 720\n",
    "        }\n",
    "\n",
    "        # Convert the column names to a format we can perform calculations on (number of minutes since 06:00:00)\n",
    "\n",
    "        stamps = self.training_set.columns[4:220]\n",
    "        \n",
    "        times = pd.to_timedelta(stamps).total_seconds()/60  # Convert to minutes\n",
    "\n",
    "        # Apply the aggregation function over specified time windows\n",
    "        for window_name, minutes in time_windows.items():\n",
    "            # Find the time range for each window\n",
    "            max_time = times.max()\n",
    "            min_time = max_time - minutes\n",
    "\n",
    "            # Get columns that fall within the current time window\n",
    "            columns_to_aggregate = [time for time in stamps if min_time < pd.to_timedelta(time).total_seconds()/60 <= max_time]\n",
    "\n",
    "            # For FFT, handle real and imaginary parts separately\n",
    "            if func == self.fft:\n",
    "                # Convert DataFrames to arrays before applying FFT\n",
    "                fft_results_train = np.apply_along_axis(func, 1, self.training_set[columns_to_aggregate].values)\n",
    "                fft_results_valid = np.apply_along_axis(func, 1, self.validation_set[columns_to_aggregate].values)\n",
    "                fft_results_test = np.apply_along_axis(func, 1, self.test_set[columns_to_aggregate].values)\n",
    "\n",
    "                for i in range(fft_results_train.shape[1]):\n",
    "                    self.training_set[f'{func_name}_{window_name}_real_{i}'] = fft_results_train[:, i].real\n",
    "                    self.training_set[f'{func_name}_{window_name}_imag_{i}'] = fft_results_train[:, i].imag\n",
    "                    self.validation_set[f'{func_name}_{window_name}_real_{i}'] = fft_results_valid[:, i].real\n",
    "                    self.validation_set[f'{func_name}_{window_name}_imag_{i}'] = fft_results_valid[:, i].imag\n",
    "                    self.test_set[f'{func_name}_{window_name}_real_{i}'] = fft_results_test[:, i].real\n",
    "                    self.test_set[f'{func_name}_{window_name}_imag_{i}'] = fft_results_test[:, i].imag\n",
    "            \n",
    "            else:\n",
    "                # Apply the function to the selected columns and store in a new column\n",
    "                self.training_set[f'{func_name}_{window_name}'] = self.training_set[columns_to_aggregate].apply(func, axis=1)\n",
    "                self.validation_set[f'{func_name}_{window_name}'] = self.validation_set[columns_to_aggregate].apply(func, axis=1)\n",
    "                self.test_set[f'{func_name}_{window_name}'] = self.test_set[columns_to_aggregate].apply(func, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/zxy_nmpx04gg6ssx89r9gl5h0000gn/T/ipykernel_724/201553568.py:3: DtypeWarning: Columns (220) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.training_set = pd.read_csv(\"cleaned_data/train_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "test = FeatureEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>corresponding_day</th>\n",
       "      <th>06:00:00</th>\n",
       "      <th>06:05:00</th>\n",
       "      <th>06:10:00</th>\n",
       "      <th>06:15:00</th>\n",
       "      <th>06:20:00</th>\n",
       "      <th>06:25:00</th>\n",
       "      <th>...</th>\n",
       "      <th>23:45:00</th>\n",
       "      <th>23:50:00</th>\n",
       "      <th>23:55:00</th>\n",
       "      <th>hypo</th>\n",
       "      <th>tar_last_10_minutes</th>\n",
       "      <th>tar_last_30_minutes</th>\n",
       "      <th>tar_last_1_hour</th>\n",
       "      <th>tar_last_3_hours</th>\n",
       "      <th>tar_last_6_hours</th>\n",
       "      <th>tar_last_12_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1514</td>\n",
       "      <td>1514</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-04-23</td>\n",
       "      <td>125.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>...</td>\n",
       "      <td>186.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.569444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1515</td>\n",
       "      <td>1515</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>154.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>...</td>\n",
       "      <td>165.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.263889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1516</td>\n",
       "      <td>1516</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-04-25</td>\n",
       "      <td>115.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>162.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1517</td>\n",
       "      <td>1517</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-04-26</td>\n",
       "      <td>227.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>...</td>\n",
       "      <td>322.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1518</td>\n",
       "      <td>1518</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-04-27</td>\n",
       "      <td>172.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>...</td>\n",
       "      <td>126.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.534722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index  id corresponding_day  06:00:00  06:05:00  06:10:00  \\\n",
       "0        1514   1514  15        2015-04-23     125.0     134.0     131.0   \n",
       "1        1515   1515  15        2015-04-24     154.0     156.0     159.0   \n",
       "2        1516   1516  15        2015-04-25     115.0     108.0      96.0   \n",
       "3        1517   1517  15        2015-04-26     227.0     228.0     229.0   \n",
       "4        1518   1518  15        2015-04-27     172.0     181.0     178.0   \n",
       "\n",
       "   06:15:00  06:20:00  06:25:00  ...  23:45:00  23:50:00  23:55:00   hypo  \\\n",
       "0     142.0     157.0     169.0  ...     186.0     180.0     173.0  False   \n",
       "1     158.0     161.0     143.0  ...     165.0     166.0     165.0   True   \n",
       "2      86.0      82.0      87.0  ...     162.0     165.0     167.0   True   \n",
       "3     231.0     240.0     244.0  ...     322.0     326.0     316.0   True   \n",
       "4     172.0     189.0     195.0  ...     126.0     123.0     115.0  False   \n",
       "\n",
       "   tar_last_10_minutes  tar_last_30_minutes  tar_last_1_hour  \\\n",
       "0                  0.0             0.666667         0.833333   \n",
       "1                  0.0             0.000000         0.000000   \n",
       "2                  0.0             0.000000         0.000000   \n",
       "3                  1.0             1.000000         1.000000   \n",
       "4                  0.0             0.000000         0.000000   \n",
       "\n",
       "   tar_last_3_hours  tar_last_6_hours  tar_last_12_hours  \n",
       "0          0.944444          0.638889           0.569444  \n",
       "1          0.527778          0.527778           0.263889  \n",
       "2          0.000000          0.069444           0.125000  \n",
       "3          1.000000          0.680556           0.625000  \n",
       "4          0.583333          0.444444           0.534722  \n",
       "\n",
       "[5 rows x 227 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "2. Add Morlet Mexican Hat Columns\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
