{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "from category_encoders import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngine:\n",
    "    def __init__(self,):\n",
    "        self.training_set = pd.read_csv(\"cleaned_data/train_data.csv\")\n",
    "        self.validation_set = pd.read_csv(\"cleaned_data/validation_data.csv\")\n",
    "        self.test_set = pd.read_csv(\"cleaned_data/test_data.csv\")\n",
    "        self.demographic_set = pd.read_csv(\"Data Tables/HScreening.txt\", delimiter = '|')\n",
    "        #self.time_series = pd.read_csv(\"Experimental_Notebooks/resampled_day.csv\")\n",
    "        #self.aggregate_window(self.tar, \"tar\")\n",
    "        #self.aggregate_window(self.tbr, \"tbr\")\n",
    "        #self.aggregate_window(self.rolling_mean, \"mean\")\n",
    "        #self.aggregate_window(self.rolling_deviation, \"std\")\n",
    "        #self.aggregate_window(self.tir, \"tir\")\n",
    "        self.aggregate_window(self.fft, \"fft\")\n",
    "        #Ã¥self.add_demographics()\n",
    "\n",
    "\n",
    "    \n",
    "    def fft(self, arr:np.ndarray) -> np.ndarray:\n",
    "        return fft(arr)\n",
    "    \n",
    "    def rolling_deviation(self, arr:np.ndarray) -> np.ndarray:\n",
    "        return np.std(arr)\n",
    "    \n",
    "    def tar(self, arr: np.ndarray) -> np.ndarray:\n",
    "        mask = arr > 180\n",
    "        return np.mean(mask)\n",
    "    \n",
    "    def tir(self, arr: np.ndarray) -> np.ndarray:\n",
    "        mask = (arr >= 70) & (arr <= 180)\n",
    "        return np.mean(mask)\n",
    "    \n",
    "    def tbr(self, arr: np.ndarray) -> np.ndarray:\n",
    "        mask = arr < 70\n",
    "        return np.mean(mask)\n",
    "    \n",
    "    def rolling_mean(self, arr: np.ndarray) -> np.ndarray:\n",
    "        return np.mean(arr)\n",
    "\n",
    "    def add_demographics(self) -> None:\n",
    "        \"\"\"\n",
    "        Merge selected demographic features into the training, validation, and test sets.\n",
    "        Encode them using one-hot encoding.\n",
    "        \"\"\"\n",
    "        # Columns to merge\n",
    "        columns_to_merge = [\n",
    "            'PtID', 'Gender', 'Ethnicity', 'Race', 'SHMostRec', 'SHNumLast12Mon', 'DKAMostRec', 'DKANumLast12Mon',\n",
    "            'OthGlucLowerMed', 'Weight', 'Height', 'PEAbnormal'\n",
    "        ]\n",
    "\n",
    "        # Filter the demographic set to include only the necessary columns\n",
    "        demographics = self.demographic_set[columns_to_merge]\n",
    "\n",
    "        # Prepare the encoder\n",
    "        encoder = OneHotEncoder(cols=['Gender', 'Ethnicity', 'Race', 'SHMostRec', 'SHNumLast12Mon', 'DKAMostRec', 'DKANumLast12Mon', 'OthGlucLowerMed', 'PEAbnormal'], use_cat_names=True)\n",
    "\n",
    "        # Fit and transform the encoder on the demographic data\n",
    "        demographics_encoded = encoder.fit_transform(demographics)\n",
    "\n",
    "        # Merge the encoded demographic data with each dataset\n",
    "        self.training_set = self.training_set.merge(demographics_encoded, how='left', left_on='id', right_on='PtID')\n",
    "        self.validation_set = self.validation_set.merge(demographics_encoded, how='left', left_on='id', right_on='PtID')\n",
    "        self.test_set = self.test_set.merge(demographics_encoded, how='left', left_on='id', right_on='PtID')\n",
    "\n",
    "    def aggregate_window(self, func: Callable, func_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Apply func over various window sizes and add columns to training and validation sets for the output of these aggregate functions\n",
    "        \n",
    "        Args:\n",
    "        func (Callable): A function to apply to each aggregate window.\n",
    "        func_name (str): The name of the function, used to create new column names.\n",
    "        \"\"\"\n",
    "        # Define the time intervals in minutes for aggregation\n",
    "        time_windows = {\n",
    "            'last_10_minutes': 10,\n",
    "            'last_30_minutes': 30,\n",
    "            'last_1_hour': 60,\n",
    "            'last_3_hours': 180,\n",
    "            'last_6_hours': 360,\n",
    "            'last_12_hours': 720\n",
    "        }\n",
    "\n",
    "        # Convert the column names to a format we can perform calculations on (number of minutes since 06:00:00)\n",
    "\n",
    "        stamps = self.training_set.columns[6:222]\n",
    "        \n",
    "        times = pd.to_timedelta(stamps).total_seconds()/60  # Convert to minutes\n",
    "\n",
    "        # Apply the aggregation function over specified time windows\n",
    "        for window_name, minutes in time_windows.items():\n",
    "            # Find the time range for each window\n",
    "            max_time = times.max()\n",
    "            min_time = max_time - minutes\n",
    "\n",
    "            # Get columns that fall within the current time window\n",
    "            columns_to_aggregate = [time for time in stamps if min_time < pd.to_timedelta(time).total_seconds()/60 <= max_time]\n",
    "\n",
    "            if func == self.fft:\n",
    "                # Process FFT and store results in new DataFrames\n",
    "                fft_data_train = {}\n",
    "                fft_data_valid = {}\n",
    "                fft_data_test = {}\n",
    "\n",
    "                fft_results_train = np.apply_along_axis(func, 1, self.training_set[columns_to_aggregate].values)\n",
    "                fft_results_valid = np.apply_along_axis(func, 1, self.validation_set[columns_to_aggregate].values)\n",
    "                fft_results_test = np.apply_along_axis(func, 1, self.test_set[columns_to_aggregate].values)\n",
    "\n",
    "                for i in range(fft_results_train.shape[1]):\n",
    "                    fft_data_train[f'{func_name}_{window_name}_real_{i}'] = fft_results_train[:, i].real\n",
    "                    fft_data_train[f'{func_name}_{window_name}_imag_{i}'] = fft_results_train[:, i].imag\n",
    "                    fft_data_valid[f'{func_name}_{window_name}_real_{i}'] = fft_results_valid[:, i].real\n",
    "                    fft_data_valid[f'{func_name}_{window_name}_imag_{i}'] = fft_results_valid[:, i].imag\n",
    "                    fft_data_test[f'{func_name}_{window_name}_real_{i}'] = fft_results_test[:, i].real\n",
    "                    fft_data_test[f'{func_name}_{window_name}_imag_{i}'] = fft_results_test[:, i].imag\n",
    "                \n",
    "                # Convert dictionary to DataFrame and concatenate\n",
    "                new_train_df = pd.DataFrame(fft_data_train)\n",
    "                new_valid_df = pd.DataFrame(fft_data_valid)\n",
    "                new_test_df = pd.DataFrame(fft_data_test)\n",
    "\n",
    "                self.training_set = pd.concat([self.training_set, new_train_df], axis=1)\n",
    "                self.validation_set = pd.concat([self.validation_set, new_valid_df], axis=1)\n",
    "                self.test_set = pd.concat([self.test_set, new_test_df], axis=1)\n",
    "                \n",
    "            else:\n",
    "                # Apply the function to the selected columns and store in a new column\n",
    "                self.training_set[f'{func_name}_{window_name}'] = self.training_set[columns_to_aggregate].apply(func, axis=1)\n",
    "                self.validation_set[f'{func_name}_{window_name}'] = self.validation_set[columns_to_aggregate].apply(func, axis=1)\n",
    "                self.test_set[f'{func_name}_{window_name}'] = self.test_set[columns_to_aggregate].apply(func, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_obj = FeatureEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>corresponding_day</th>\n",
       "      <th>06:00:00</th>\n",
       "      <th>06:05:00</th>\n",
       "      <th>06:10:00</th>\n",
       "      <th>06:15:00</th>\n",
       "      <th>06:20:00</th>\n",
       "      <th>06:25:00</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_last_12_hours_real_139</th>\n",
       "      <th>fft_last_12_hours_imag_139</th>\n",
       "      <th>fft_last_12_hours_real_140</th>\n",
       "      <th>fft_last_12_hours_imag_140</th>\n",
       "      <th>fft_last_12_hours_real_141</th>\n",
       "      <th>fft_last_12_hours_imag_141</th>\n",
       "      <th>fft_last_12_hours_real_142</th>\n",
       "      <th>fft_last_12_hours_imag_142</th>\n",
       "      <th>fft_last_12_hours_real_143</th>\n",
       "      <th>fft_last_12_hours_imag_143</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1514</td>\n",
       "      <td>1514</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-04-23</td>\n",
       "      <td>125.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-375.498232</td>\n",
       "      <td>-187.338107</td>\n",
       "      <td>639.343081</td>\n",
       "      <td>-623.597227</td>\n",
       "      <td>-801.358323</td>\n",
       "      <td>246.066452</td>\n",
       "      <td>-4223.991086</td>\n",
       "      <td>-843.723887</td>\n",
       "      <td>2128.451672</td>\n",
       "      <td>-806.877055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1515</td>\n",
       "      <td>1515</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>154.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-243.474056</td>\n",
       "      <td>-121.439490</td>\n",
       "      <td>-61.100857</td>\n",
       "      <td>-540.021250</td>\n",
       "      <td>310.359535</td>\n",
       "      <td>676.772014</td>\n",
       "      <td>-388.507416</td>\n",
       "      <td>-144.056005</td>\n",
       "      <td>-965.002148</td>\n",
       "      <td>-4700.062468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1516</td>\n",
       "      <td>1516</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-04-25</td>\n",
       "      <td>115.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>213.472023</td>\n",
       "      <td>-362.105330</td>\n",
       "      <td>356.415422</td>\n",
       "      <td>-957.061897</td>\n",
       "      <td>-154.112694</td>\n",
       "      <td>1232.506617</td>\n",
       "      <td>2588.701494</td>\n",
       "      <td>-114.455035</td>\n",
       "      <td>-2129.420318</td>\n",
       "      <td>-1240.817007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1517</td>\n",
       "      <td>1517</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-04-26</td>\n",
       "      <td>227.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-962.852296</td>\n",
       "      <td>-1384.676562</td>\n",
       "      <td>392.963017</td>\n",
       "      <td>763.990430</td>\n",
       "      <td>-3625.241781</td>\n",
       "      <td>-962.510805</td>\n",
       "      <td>1392.138415</td>\n",
       "      <td>-6154.000006</td>\n",
       "      <td>1149.464359</td>\n",
       "      <td>-1165.271128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1518</td>\n",
       "      <td>1518</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-04-27</td>\n",
       "      <td>172.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>...</td>\n",
       "      <td>292.672651</td>\n",
       "      <td>231.608371</td>\n",
       "      <td>41.076141</td>\n",
       "      <td>1285.873055</td>\n",
       "      <td>-206.019620</td>\n",
       "      <td>2473.914536</td>\n",
       "      <td>-2482.348817</td>\n",
       "      <td>-413.338221</td>\n",
       "      <td>2418.116259</td>\n",
       "      <td>-691.407484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 765 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index  id corresponding_day  06:00:00  06:05:00  06:10:00  \\\n",
       "0        1514   1514  15        2015-04-23     125.0     134.0     131.0   \n",
       "1        1515   1515  15        2015-04-24     154.0     156.0     159.0   \n",
       "2        1516   1516  15        2015-04-25     115.0     108.0      96.0   \n",
       "3        1517   1517  15        2015-04-26     227.0     228.0     229.0   \n",
       "4        1518   1518  15        2015-04-27     172.0     181.0     178.0   \n",
       "\n",
       "   06:15:00  06:20:00  06:25:00  ...  fft_last_12_hours_real_139  \\\n",
       "0     142.0     157.0     169.0  ...                 -375.498232   \n",
       "1     158.0     161.0     143.0  ...                 -243.474056   \n",
       "2      86.0      82.0      87.0  ...                  213.472023   \n",
       "3     231.0     240.0     244.0  ...                 -962.852296   \n",
       "4     172.0     189.0     195.0  ...                  292.672651   \n",
       "\n",
       "   fft_last_12_hours_imag_139  fft_last_12_hours_real_140  \\\n",
       "0                 -187.338107                  639.343081   \n",
       "1                 -121.439490                  -61.100857   \n",
       "2                 -362.105330                  356.415422   \n",
       "3                -1384.676562                  392.963017   \n",
       "4                  231.608371                   41.076141   \n",
       "\n",
       "   fft_last_12_hours_imag_140  fft_last_12_hours_real_141  \\\n",
       "0                 -623.597227                 -801.358323   \n",
       "1                 -540.021250                  310.359535   \n",
       "2                 -957.061897                 -154.112694   \n",
       "3                  763.990430                -3625.241781   \n",
       "4                 1285.873055                 -206.019620   \n",
       "\n",
       "   fft_last_12_hours_imag_141  fft_last_12_hours_real_142  \\\n",
       "0                  246.066452                -4223.991086   \n",
       "1                  676.772014                 -388.507416   \n",
       "2                 1232.506617                 2588.701494   \n",
       "3                 -962.510805                 1392.138415   \n",
       "4                 2473.914536                -2482.348817   \n",
       "\n",
       "   fft_last_12_hours_imag_142  fft_last_12_hours_real_143  \\\n",
       "0                 -843.723887                 2128.451672   \n",
       "1                 -144.056005                 -965.002148   \n",
       "2                 -114.455035                -2129.420318   \n",
       "3                -6154.000006                 1149.464359   \n",
       "4                 -413.338221                 2418.116259   \n",
       "\n",
       "   fft_last_12_hours_imag_143  \n",
       "0                 -806.877055  \n",
       "1                -4700.062468  \n",
       "2                -1240.817007  \n",
       "3                -1165.271128  \n",
       "4                 -691.407484  \n",
       "\n",
       "[5 rows x 765 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_obj.test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "2. Add Morlet Mexican Hat Columns\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
